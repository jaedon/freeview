#! /usr/local/bin/python
#########################################################################
#     Copyright (c) 2003-2012, Broadcom Corporation
#     All Rights Reserved
#     Confidential Property of Broadcom Corporation
#
#  THIS SOFTWARE MAY ONLY BE USED SUBJECT TO AN EXECUTED SOFTWARE LICENSE
#  AGREEMENT  BETWEEN THE USER AND BROADCOM.  YOU HAVE NO RIGHT TO USE OR
#  EXPLOIT THIS MATERIAL EXCEPT SUBJECT TO THE TERMS OF SUCH AN AGREEMENT.
#
# $brcm_Workfile: ts_verify.py $
# $brcm_Revision: Hydra_Software_Devel/10 $
# $brcm_Date: 7/3/12 6:08p $
#
# [File Description:]
# TS Verify
#
# Tool for analyzing TS output generated by software Mux, and verifying
# certain criteria are correct
#
# Revision History:
#
# $brcm_Log: /rockford/unittests/syslib/muxlib/tests/ts_verify.py $
# 
# Hydra_Software_Devel/10   7/3/12 6:08p delkert
# SW7425-3415: Adjust userdata timing verification to match changes to
# code. Increase sync count to allow for streams with very repetitive
# data.
#
# Hydra_Software_Devel/9   6/19/12 10:22a delkert
# SW7425-3256: Fix up userdata tests for B-frames to support frame
# reordering. Add correction for encode delay.
#
# Hydra_Software_Devel/8   6/15/12 4:54p delkert
# SW7425-3214: Augment output for failure to show all userdata timing
# results
#
# Hydra_Software_Devel/7   6/15/12 12:59p delkert
# SW7425-3214: Fix method used to match userdata to video frames to allow
# for encode variances.
#
# Hydra_Software_Devel/6   6/12/12 5:45p delkert
# SW7425-3214: Add workaround for initial video OPTS/PTS divergence.
#
# Hydra_Software_Devel/5   6/11/12 5:03p delkert
# SW7425-3214: Remove debug prints
#
# Hydra_Software_Devel/4   6/11/12 3:48p delkert
# SW7425-3214: Add argument to specify results directory. Fix up handling
# of input descriptors using muxlib_input module.
# fix up processing of delta_pts lists if no values found. Add output of
# failing entries and output a timing summary.
#
# Hydra_Software_Devel/3   6/8/12 3:00p delkert
# SW7425-2567: Fix imports for CSV errors
#
# Hydra_Software_Devel/2   6/8/12 10:25a delkert
# SW7425-2567: Add PCR packet structure verification
#
# Hydra_Software_Devel/1   5/4/12 2:29p delkert
# SW7425-2567: Initial TS Mux test verification tool
#
##########################################################################
#
# Userdata Testing: verify that the userdata from specified PIDs is
# passed thru correctly, with allowed changes (such as PTS field will differ)
# and that the userdata timing is within allowed tolerances
#
# System Data Testing: verify that system data is inserted with the correct
# timing in between PCR packets
#
# PCR verification: verify the PCR packets are created properly, have the
# correct spacing, and have the correct values
#

import argparse
import os
import sys
import itertools
from math import sqrt

# FIXME: add the search path of modules we provide
import tsfile
import muxlib_input
from csv_convert import CSVConverter
from csv_convert import Error as CSVError

_version = '0.0.0.0'

MAX_PCR_DIFF = 2700000    # 100ms @ 27MHz
SCALE_PCR_TO_MS = 27000
SCALE_PTS_TO_MS = 90

class TestFail(Exception):
   pass

# action to convert each argument to int from hex if it begins with '0x'
# otherwise treats argument as int
class InputHexOrIntAction(argparse.Action):
   def _convert(self, value):
      if value.startswith('0x'):
         return int(value,16)
      else:
         return int(value)

   def __call__(self, parser, namespace, values, option_string=None):
      try:
         if type(values) is list:
            # convert each list element
            values = [self._convert(x) for x in values]
         else:
            # assume a single value
            values = self._convert(values)
      except (TypeError, ValueError):
         raise argparse.ArgumentError(self, values)
      setattr(namespace, self.dest, values)


def get_cmd_line_args():
   parser = argparse.ArgumentParser(description='TS Mux Test Result Verification')

   # arguments that apply to all the test modes (e.g. flags for controlling log output etc) ...
   parser.add_argument('--version', action='version', version='TS Verify: Ver. ' + _version)

   parser.add_argument('-stop', action='store_true', default=False, help='Stop after the first error. Otherwise continue to the end of the test')

   parser.add_argument('-debug', action='store_true', default=False, help='Turn on debug output')

   parser.add_argument('-results', metavar='result_dir', type=str, help='The directory where the generated output from the test can be found'
      'If not specified, current directory is assumed')

   # -ignore-sync to specify that sync errors should be ignored
   # -log to output results to logfile
   # -quiet to only print difference offsets in the original file(s) not the complete differences

   # subparsers for the different test modes ...
   subparsers = parser.add_subparsers(dest = 'test_mode', title='Test modes',
                                       description='The following are the different types of verification that can be performed',
                                       help='Verification Type')

   #
   # Userdata arguments
   #
   parser_userdata = subparsers.add_parser("userdata")

   parser_userdata.add_argument('src_file', metavar='source_filename', type=str, help='The source TS file used in the transcode')

   parser_userdata.add_argument('out_file', metavar='output_filename', type=str, help='The (generated) TS file to be verified')

   parser_userdata.add_argument('spid', metavar='PID', help='List of PIDs in the source file to process', nargs='+', action=InputHexOrIntAction)

   parser_userdata.add_argument('-ud', metavar='userdata_csv_file', type=str, help='The (generated) Userdata CSV file from the application under test (e.g. BMUXlib_TS_Userdata_00.csv). '
         'Required only for retimed userdata')

   # NOTE: currently, this processing supports only a single companion video stream.  If multiple video ever supported, we'd need to supply the base name of
   # the CSV and locate them based on index or instance number
   parser_userdata.add_argument('-vid', metavar='video_csv_file', type=str, help='The (generated) Video CSV file from the application under test (e.g. BMUXlib_INPUT_DESC_VIDEO_00_00.csv). '
         'Required only for retimed userdata')

   parser_userdata.add_argument('-opid', metavar='PID', type=str, help='List of PIDs in the output file to process. '
         'if not specified, the source PIDs will be assumed (i.e. no pid remapping)', nargs='+', action=InputHexOrIntAction)

   parser_userdata.add_argument('-sync', metavar='N', type=int, help='Minimum number of packets required to match for sync (default: %(default)s)', default=5);

   # -ignore-sizes to specify that it should complain if number packets in each stream differs (currently, ignores the difference in length)
   # (i.e. "Error excess data found in <source|output> file")


   #
   # System Data Arguments
   #
   parser_sysdata = subparsers.add_parser("sysdata")

   parser_sysdata.add_argument('out_file', metavar='output_filename', type=str, help='The (generated) TS file to be verified')

   parser_sysdata.add_argument('pcr_pid', help='PID containing PCRs', action=InputHexOrIntAction)

   # FIXME: what if we want to test insertion interval per-packet?
   parser_sysdata.add_argument('interval', metavar='insertion_interval', type=int, help='The interval (in ms) used for system data insertion')

   parser_sysdata.add_argument('pids', metavar='sd_pid', help='List of PIDs containing the system data to verify', nargs='+', action=InputHexOrIntAction)
   #
   # PCR Verification arguments
   #
   parser_pcr = subparsers.add_parser("pcr")

   parser_pcr.add_argument('file', metavar='test_filename', type=str, help='The TS file to verify PCR')

   parser_pcr.add_argument('pid', metavar='pcr_pid', help='PID containing the PCR to verify.  If not specified, an attempt will be made '
       ' to automatically determine the PID', action=InputHexOrIntAction)

   parser_pcr.add_argument('expected', metavar='expected_interval', type=int, help='The PCR interval expected between PCR values (in ms)')

   return parser.parse_args()

# if a test fails, and -stop specified, then bail out immediately
# else print a message and continue with the rest of the tests
def test_fail(reason):
   if args.stop:
      raise TestFail(reason)
   else:
      print reason
      return False

#
# Verify that the PCRs in the output are correct
# - Generated values should increment by the specified interval
# - PCR jitter should not exceed some maximum (currently 1% of mean)
#
# NOTE: since the mux generates variable-rate TS files, we cannot
# verify the PCR packet spacing.
#
# NOTE: This code currently does not verify discontinuities
# Data generated by the mux should not have discontinuities in PCR
#
def verify_pcr():
   test_result = True

   try:
      source = tsfile.open(args.file)
   except IOError:
      print "Unable to Open Output File: %s!" % args.file
      return False

   if args.pid is None:
      # FIXME: attempt to locate the PAT/PMT? OR find the first packet with a PCR and use its PID
      print "No PID specified!"
      return False


   print "Extracting PCR information from PID %s ..." % args.pid,
   source.set_filter({'pid':args.pid, 'pcr_flag':True})
   pcr_packets = [x for x in source]
   if not pcr_packets:
      print "No PCRs found"
      return False
   else:
      print "%s PCRs found" % len(pcr_packets)
   # ideally, we'd like a large enough set of packets to provide a representative sample
   if len(pcr_packets) < 20:
      print "Warning!! Not enough PCR packets to give measurement precision"

   print "Verifying PCR packet structure...",
   # The mux creates PCR packets that contain NO other data
   # has a PID, adaptation header, adapt_len = 183, pcr present = true
   # all other data (in the adaptation header) is 0xFF
   # NOTE: can't verify continuity count, since it wont increment for packets without payload
   for packet in pcr_packets:
      if packet.payload_present:
         test_result = test_fail("Payload found - PCR packets should not have a payload")
      if not packet.adaptation_present:
         test_result = test_fail("Adaptation Header is missing")
      if packet.adaptation_length != 183:
         test_result = test_fail("Invalid Adapation Header Length: %s (expecting 183)" % packet.adaptation_length)
      if packet.adaptation_data != ((chr(0xFF) * (packet.adaptation_length-7))):
         test_result = test_fail("Packet Adaptation Data invalid (expecting all 0xFF)")
   if test_result:
      print "OK"

   pcr_pairs = tsfile.pairwise(pcr_packets)
   intervals = [y.pcr-x.pcr for x,y in pcr_pairs]

   avg_interval = sum(intervals)/float(len(intervals))
   max_interval = max(intervals)
   min_interval = min(intervals)

   # std. deviation is a measure of the amount of PCR jitter we have...
   variance = sum([(x - avg_interval)**2 for x in intervals]) / (len(intervals)-1)
   stddev = sqrt(variance)

   print "PCR Intervals: "
   print "Avg: %.2f (%.2f ms)" % (avg_interval, avg_interval / SCALE_PCR_TO_MS)
   print "Max: %s (%s ms)" % (max_interval, max_interval / SCALE_PCR_TO_MS)
   print "Min: %s (%s ms)" % (min_interval, min_interval / SCALE_PCR_TO_MS)
   print "Std. Dev.: %.2f (%.1f%% of mean)" % (stddev, stddev*100/avg_interval)

   # average interval (mean) should be within 1 % of the expected value
   expected = args.expected * SCALE_PCR_TO_MS
   diff = abs(avg_interval - expected)
   if diff > expected * 0.01:
      test_result = test_fail(u"Average PCR interval is not within expected bounds [%s ms \u00b11%%]" % args.expected)

   # ideally, in a generated stream, we would not expect deviation to exceed 1% of mean
   # (in fact, for the most part we expect deviation to be zero!)
   if stddev > (avg_interval * 0.01):
      test_result = test_fail("Std. Dev Exceeds 1% of mean!")

   # FIXME: we should also test PCR relationship to PTS and DTS of the streams to ensure that nothing crosses
   # and that their relative timings are consistent??

   return test_result

# userdata verification requires checking that the userdata information has been
# passed thru from source TS file to output TS file transparently for the
# indicated PIDs with the exception of altered timing (PTS/DTS may differ).

# Userdata testing should also verify the packet timing (pacing/packet intervals),
# and delta from target video frame
#
def verify_userdata():
   test_result = True      # test passes until determined otherwise
   pes_filter = ['pts', 'dts']
   ts_filter = ['pid', 'payload']
   video_desc = userdata_timing = None

   print "Running Userdata test...\n"
   results_dir = args.results or "."

   try:
      # open the source file
      source = tsfile.open(args.src_file)
   except IOError:
      return test_fail("Unable to Open Source File: %s!" % args.src_file)

   fname = os.path.join(results_dir, args.out_file)
   try:
      # open the output file
      outfile = tsfile.open(fname)
   except IOError:
      return test_fail("Unable to Open Output File: %s!" % fname)

   # if timing files provided, verify they are present, and open them
   if args.vid is not None:
      try:
         vid_file = muxlib_input.DescriptorFile(results_dir, 'video', 0)
      except IOError:
         return test_fail("Unable to Open Video Timing File: %s!" % args.vid)
      except muxlib_input.Error, e:
         return test_fail("Unable to process video timing file")
      try:
         # NOTE: we're only interested in "frame" descriptors
         video_desc = [x for x in vid_file if x.is_frame_start]
      except muxlib_input.Error, e:
         return  test_fail(e)

      # prune initial descriptors for which vOPTS is 0 (blank frames) or not RAP
      video_desc = [x for x in itertools.dropwhile(lambda x: (x.opts == 0) or (not x.is_rap), video_desc)]
      vdpts = [y.pts - x.pts for x,y in tsfile.pairwise(video_desc)]
      video_dpts = sum(vdpts)/len(vdpts)

      # calculate the target PTS for the userdata, and the corresponding video PTS
      # (i.e. using the adjustment that would've been applied based on the video frame's OPTS/PTS)
      # this same timing information is used for testing all userdata pids ...
      prev_opts = 0
      video_timing = []
      for video in video_desc:
         # NOTE: timing adjustment only changes when vopts changes and only on RAP
         # NOTE: since we prune off initial values with vopts=0, we guarantee
         # timing adjustment will be set on the first iteration
         if (video.opts != prev_opts) and video.is_rap:
            timing_adjust = video.opts - video.pts
            prev_opts = video.opts
         target_pts = video.dts + timing_adjust
         video_timing.append({'target':target_pts,'pts':video.pts,'dts':video.dts})
      # determine initial DTS/PTS offset from the encoder
      dts_pts_offset = [desc.pts - desc.dts for desc in video_desc if desc.is_rap][0]

   if args.ud is not None:
      fname = os.path.join(results_dir, args.ud)
      try:
         ud = open(fname, "r")
         ud_reader = CSVConverter(ud, {'dts_present':bool})
         userdata_timing = [x for x in ud_reader]
      except IOError:
         return test_fail("Unable to Open Userdata Timing File: %s!" % fname)
      except CSVError, e:
         return test_fail('file %s, line %d: %s' % (fname, ud_reader.line_num, e))

   src_pids = args.spid
   out_pids = (args.opid if args.opid else args.spid)

   # prune excess pids since the zip() below will ignore them anyway - no
   # point extracting pids that will never be used
   if len(src_pids) > len(out_pids):
      print "Ignoring excess Source PIDs supplied: %s\n" % " ".join(str(x) for x in src_pids[len(out_pids):len(src_pids)])
      src_pids = src_pids[:len(out_pids)]
   if len(out_pids) > len(src_pids):
      print "Ignoring excess Output PIDs supplied: %s\n" % " ".join(str(x) for x in out_pids[len(src_pids):len(out_pids)])
      out_pids = out_pids[:len(src_pids)]

   print "Extracting Source PIDs %s ..." % src_pids
   source_streams = source.extract_pids(*src_pids)
   # FIXME: need to check for sync errors etc

   for pid in src_pids:
      if not source_streams[pid]:
         test_result = test_fail("PID %s not found!" % pid)
      else:
         print "PID %s: %s packets extracted." % (pid, len(source_streams[pid]))

   print
   print "Extracting Output PIDs %s ..." % out_pids
   output_streams = outfile.extract_pids(*out_pids)
   # FIXME: need to check for sync errors etc

   for pid in out_pids:
      if not output_streams[pid]:
         test_result = test_fail("PID %s not found!" % pid)
      else:
         print "PID %s: %s packets extracted." % (pid, len(output_streams[pid]))

   # we don't know if the CSV files are required until we have found PTS values in the PIDs
   # thus, dPTS extraction etc needs to be done in the PID loop.

   # for each PID pair to verify...
   for spid, opid in zip(src_pids, out_pids):
      print
      print "==== Processing PIDs: %s -> %s ====" % (spid, opid)

      source_packets = source_streams[spid]
      output_packets = output_streams[opid]

      # incase -stop not supplied, and a pid contains no data ...
      if not source_packets or not output_packets:
         # skip this test iteration - no point processing this pid-pair
         test_result = test_fail("Skipping PIDs %s -> %s - no data to process" % (spid, opid))
         continue

      print "Syncing TS streams ...",
      # NOTE: we cannot simply use the index of the first PES packet, since non-PES data
      # may precede the first PES
      # Ensure we sync to at least 10% of the available TS packets
      sync_count = max(min(len(source_packets)/10, len(output_packets)/10), args.sync)
      ts_sync_index = tsfile.sync_to(source_packets, output_packets, ts_filter, sync_count)
      if ts_sync_index is None:
         test_result = test_fail("Unable to sync source to output")
         continue
      else:
         print "OK"
      if args.debug:
         print "Sync @ packet: %s" % source_packets[ts_sync_index].packet_index

      print "Comparing TS Content ... ",
      # compare the original TS packets, excluding the payload (the PES data will be
      # compared separately, below).  This ensures the original TS headers are left intact
      both_streams = zip(source_packets[ts_sync_index:], output_packets)
      diffs = [diff for diff in [src_pkt.compare(out_pkt, ts_filter) for (src_pkt, out_pkt) in both_streams] if diff]
      if not diffs:
         print "%s TS packets compare OK" % len(both_streams)
      else:
         test_result = test_fail("%s Difference%s Found:" % (len(diffs), ("" if len(diffs) == 1 else "s")))
         # FIXME: this needs to display the offsets (within the original files) of the differences
         for diff in diffs:
            print diff

      # FIXME: is it a requirement that we have PES present - can userdata consist of just TS packets???
      print "Extracting PES from PIDs: "
      source_pes = [x for x in tsfile.PESReader(source_packets)]
      if not source_pes:
         # skip this test iteration - no PES data to process
         test_result = test_fail("Source[%s] contains no PES data!" % spid)
         continue
      else:
         print "Source[%s]: %s PES packets extracted." % (spid, len(source_pes))

      out_pes = [x for x in tsfile.PESReader(output_packets)]
      if not out_pes:
         test_result = test_fail("Output[%s] contains no PES data!" % opid)
         continue
      else:
         print "Output[%s]: %s PES packets extracted." % (opid, len(out_pes))
      # FIXME: need to check for sync errors

      print "Syncing PES streams ...",
      # sync the source packets to the output packets
      # NOTE: Ensure we sync at least 10% of the available packets. Some userdata
      # streams have been found to have content that is very repetitive
      sync_count = max(min(len(source_pes)/10, len(out_pes)/10), args.sync)
      pes_sync_index = tsfile.sync_to(source_pes, out_pes, pes_filter, sync_count)  # index of sync location in the (source) PES list
      if pes_sync_index is None:
         test_result = test_fail("Unable to sync source to output")
         continue
      else:
         print "OK"
      if args.debug:
         print "Sync @ packet: %s" % source_pes[pes_sync_index].packet_index

      # compare the packet in the source with the packet in the output from the sync point onward, excluding the pts field
      # FIXME: This will automatically ignore diffs at the end of the file (i.e. if one file is shorter than the other)
      print "Comparing PES Content ... ",
      both_streams = zip(source_pes[pes_sync_index:], out_pes)
      # FIXME: To keep the original index into the stream, keep this diff whole (don't compress it)
      # then we can use the index of the diff as the index to the packet (in both_streams) => gets us the index of the PES packet in both_streams
      # How do we get the original TS packet?
      diffs = [diff for diff in [src_pkt.compare(out_pkt, pes_filter) for (src_pkt, out_pkt) in both_streams] if diff]
      if not diffs:
         print "%s PES packets compare OK" % len(both_streams)
      else:
         test_result = test_fail("%s Difference%s Found:" % (len(diffs), ("" if len(diffs) == 1 else "s")))
         for diff in diffs:
            print source_packets[diff['pkt_index'][0]].packet_index, output_packets[diff['pkt_index'][1]].packet_index
            print diff

      # if this PID has timing data, verify the PES timing...
      timed_pes = [x for x in out_pes if x.pts_present]
      if not timed_pes:
         continue

      print "Verifying PES timing ...",
      # ensure that the PTS of the userdata is within 2 frames of the video frame it belongs to ...

      if video_desc is None or userdata_timing is None:
         test_result = test_fail("Timing data not provided: Please specify -ud and -vid arguments");
         continue

      # extract the userdata timing associated with this pid ...
      ud_timing = [x for x in userdata_timing if x['pid'] == spid]

      # the PES pts should match the data from the userdata timing file
      # - if not, it means that not all retimed userdata was sent to the transport!
      # NOTE: the zip() will ignore trailing values that were not muxed due to finishing pipeline
      retimed_pes_ok = [x['new_pts'] == y.pts for x,y in zip(ud_timing, timed_pes)]
      if not retimed_pes_ok or not all(retimed_pes_ok):
         test_result = test_fail("Userdata timing does not match PES - not all userdata was muxed")
         continue

      # FIXME: check UD PTS are in increasing order

      # NOTE: number of userdata PES packets in output file may be less than number of userdata entries in
      # the userdata timing CSV file (due to userdata that is still in the pending queue when the transcode ended)
      # thus, we don't need to compare these values so remove them from the userdata timing
      if len(ud_timing) > len(both_streams):
         del ud_timing[len(both_streams):]

      # find video frame that this userdata belongs to ...

      # NOTE: we can't match video based on OPTS due to frame reordering.  It needs to be done based on DTS.
      # Userdata is matched to a target PTS that is generated using video DTS + adjustment
      # where adjustment is: video OPTS - video PTS

      # find the video frame that matches the userdata by matching its target PTS
      # (find the video frames for which: 0 < userdata OPTS - target PTS < 3 dPTS)
      # use the first of these that matches (search only from the last matched frame forward
      # -i.e. that same frame can be matched more than once)
      curr_vid_index = 0
      matched_userdata = []
      for ud in ud_timing:
         for vid in video_timing[curr_vid_index:]:
            diff = ud['opts'] - vid['target']
            if diff > 0 and diff < (3 * video_dpts):
               # allow for the initial PTS/DTS offset since we are comparing with video DTS not PTS
               delta = abs(ud['new_pts'] - vid['dts']) - dts_pts_offset
               matched_userdata.append((ud,vid,delta))
               curr_vid_index = video_timing.index(vid)
               break

      if len(matched_userdata) != len(ud_timing):
         test_result = test_fail("Unable to match all userdata to video frames")
         continue

      target = (2 * video_dpts)
      failing = [info for info in matched_userdata if info[2] > target]
      if failing:
         test_result = test_fail("\nUserdata timing exceeds 2 dPTS (%s) variation from original video" % target)
         if args.debug:
            print "Results:"
            for (ud,vid,delta) in matched_userdata:
               print "%s ud: %s/%s vid: %s/%s (delta: %s)" % (("FAIL" if delta > target else " "*4),ud['opts']/2,ud['new_pts']/2,vid['target']/2,vid['pts']/2, delta/2)
      else:
         print "OK"

      # drop initial outliers (can happen - sometimes first delta is -ve due to frame match)
      vid_pts_deltas = [info[2] for info in itertools.dropwhile(lambda x: x[2] < 0, matched_userdata)]

      avg_dpts = sum(vid_pts_deltas)/float(len(vid_pts_deltas))
      max_dpts = max(vid_pts_deltas)
      min_dpts = min(vid_pts_deltas)
      # std. deviation is a measure of the amount of timing jitter we have...
      variance = sum([(x - avg_dpts)**2 for x in vid_pts_deltas]) / (len(vid_pts_deltas)-1)
      stddev = sqrt(variance)

      print
      print "Timing Summary: (2dPTS = %s)" % (2*video_dpts)
      print "Avg: %.2f (%.2f ms)" % (avg_dpts, avg_dpts / SCALE_PTS_TO_MS)
      print "Max: %s (%s ms)" % (max_dpts, max_dpts / SCALE_PTS_TO_MS)
      print "Min: %s (%s ms)" % (min_dpts, min_dpts / SCALE_PTS_TO_MS)
      print "Std. Dev.: %.2f (%.1f%% of mean)" % (stddev, stddev*100/avg_dpts)

   print
   return test_result

#
# verify that the specified PIDs contain the expected system data
# and verify that the system data packets are inserted in the required intervals within the stream
#
def verify_sysdata():
   try:
      # open the output file
      outfile = tsfile.open(args.out_file)
   except IOError:
      print "Unable to Open Output File: %s!" % args.out_file
      return False

   # read in the system data packets
   sys_data_packets = outfile.findall({ 'pid': args.pids })
   sd_packet_indicies = [x.packet_index for x in sys_data_packets]
   outfile.rewind()
   pcr_packets = [x for x in outfile.findall({'pid':args.pcr_pid}) if x.pcr_flag]
   pcr_intervals = [(x.packet_index, y.packet_index, y.pcr - x.pcr) for x,y in tsfile.pairwise(pcr_packets)]
   interval = args.interval * SCALE_PCR_TO_MS
   current = 0
   # FIXME: how does the test know how many packets were inserted?
   # if sys data read from file, we know the number of entries, otherwise have to know in advance, or be told via command-line
   remaining = 100
   for start_index,end_index,pcr_interval in pcr_intervals:
      expected = min(pcr_interval / interval, remaining)
      count = len([x for x in sys_data_packets[current:] if (x.packet_index > start_index) and (x.packet_index < end_index)])
      # FIXME: what about other system data packets - do they "push out" the packets that need to be inserted?
      # for the initial interval, last scheduled time is actually the current insertion time, which is the time AFTER
      # the PAT/PMT have been inserted.  Thus, the expected will not fit in the first interval!
      # Do we need a CSV containing the locations of all (other) system data packets?
      # alternatively, we need to count packets
      # if the insertion interval is small enough, the PAT PMT may account for more than one SD packet
      # if we count packets we need to know SD bitrate to know how many packets exist in the desired interval

      # read the system data CSV - indicates the system data inserted and its interval
      # => NOTE no way to know which PID the sys data packet is for?
      # we can know the data, its length and inserton interval
      if count != expected:
         print "Error! Expecting %s system data packets, got %s" % (expected, count)
      current += count
      remaining -= count
      if current >= len(sys_data_packets):
         break

   #verify the system data content is correct
   print "NOT IMPLEMENTED"
   return True

if __name__ == '__main__':
   args = get_cmd_line_args()
   test_result = False
   print

   # FIXME: add BIST - predefined modes that exercise the failure cases in tthe tests defined
   # i.e. failure to sync, packet differences, timing failures, etc.
   if args.test_mode == 'userdata':
      try:
         test_result = verify_userdata()
      except TestFail, reason:
         print "Userdata Test aborted: %s" % reason
   elif args.test_mode == 'sysdata':
      try:
         test_result = verify_sysdata()
      except TestFail, reason:
         print "System Data Test aborted: %s" % reason
   elif args.test_mode == 'pcr':
      try:
         test_result = verify_pcr()
      except TestFail, reason:
         print "PCR Test aborted: %s" % reason

   print ("Test OK" if test_result else "Test FAILED!")
   sys.exit(not test_result)

   # TO DO:
   # Provide logging for test results
   # Print locations of differences as offsets in the original file (or perhaps options to output diff locations as packet indexes etc.)
   # progress status of PID extraction (since this is typically the longest step)?

   # USERDATA TODO:
   #   Userdata should test: pids with PTS, pids with no PTS, pids with adaptation data, pids with no adaptation data, pids with adaptation data and payload
   #      multiple PIDs (at least PID with PTS, pid without PTS). Userdata with discontinuities (to future time, and to past time)
   #   Verify userdata insertion is given priority over system data?
   #   Verify userdata insertion done in round-robin?
   #   Verify userdata insertion where not all packets fit in the current PCR interval (and that it picks up with the next input in order).?
   #   Verify userdata insertion where combined data rate exceeds overall system data rate (drop packets?)

   #
   #System data: config would be: the packet data of the system data inserted (and if that data is simply repeated, then how many packets), and the insertion interval
   #            For system data we need to be able to test: single packet, multi-packet (to ensure that splits across PCR boundaries happen properly), integral number
   #               of sys data packets per PCR interval (requires knowing the PCR interval), packet interval such that non-integer number of packets fit (i.e. we should
   #               have ping-pong effect)
